# -*- coding: utf-8 -*-
"""dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/171VT0vprvoid028yiWeUt3qmgZ54RMan

https://levelup.gitconnected.com/graph-convolutional-network-node-classification-with-tensorflow-49d3e091ea15
https://github.com/tkipf/gcn/tree/master/gcn
"""

import numpy as np
import torch
from google.colab import drive
import scipy.sparse as sp

train_size = 3 # Number to divide dataset by for train, valid and test datasets

"""Load facebook data

"""

def load_data(path='/content/gdrive', file='/MyDrive/Colab Notebooks/Lab 3/facebook.npz'):
  drive.mount('/content/gdrive', force_remount=True);
  data = np.load("{}{}".format(path, file))
  return data

data = load_data()

def process_data(dataset):
  train_ds = range(0,round(data['features'].shape[0]/3))
  valid_ds = range(round(data['features'].shape[0]/3), round(2*data['features'].shape[0]/3))
  test_ds = range(round(2*data['features'].shape[0]/3), round(2*data['features'].shape[0]))

  train_ds = torch.LongTensor(train_ds)
  valid_ds = torch.LongTensor(valid_ds)
  test_ds = torch.LongTensor(test_ds)

  features = torch.as_tensor(data['features'])
  features = features[0:round(features.size(dim = 0)/3), :]
  target = torch.from_numpy(data['target'])
  edges = torch.from_numpy(data['edges'])

  return train_ds, valid_ds, test_ds, features, target, edges

def normalize(mx):
  r_inv = np.power(np.array(mx.sum(1)), -1).flatten()
  r_inv[np.isinf(r_inv)] = 0.
  r_mat_inv = sp.diags(r_inv)
  mx = r_mat_inv.dot(mx)
  return mx

def sparse_mx_to_sparse_tensor(mx):
  mx = mx.tocoo().astype(np.float32)
  indices = torch.from_numpy(
      np.vstack((mx.row, mx.col)).astype(np.int64))
  values = torch.from_numpy(mx.data)
  shape = torch.Size(mx.shape)
  return torch.sparse.FloatTensor(indices, values, shape)

def adj_matrix(data):
  edge_size = round(data['edges'].shape[0]/train_size)
  target_size = round(data['target'].shape[0]/train_size)
  #print(data['edges'].shape[0].max())
  print(edge_size, target_size)

  adj = sp.coo_matrix((np.ones(data['edges'].shape[0]), 
                       (data['edges'][0:data['edges'].shape[0], 0], 
                        data['edges'][0:data['edges'].shape[0], 1])),
                      shape=(data['target'].shape[0], data['target'].shape[0]), 
                      dtype=np.float32)
  adj.resize((target_size, target_size))
  #DEBUG
  edge_size = round(data['edges'].shape[0])
  target_size = round(data['target'].shape[0])
  #DEBUG END
  adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)
  adj = normalize(adj + sp.eye(adj.shape[0]))
  adj = sparse_mx_to_sparse_tensor(adj)
  return adj

train_ds, valid_ds, test_ds, features, target, edges = process_data(data)
for item in data.files:
    print(item)
    print(data[item])
    print(data[item].size)

for n in range(data['target'].size):
  print(data['target'][n])

train, valid, test = process_data(data)
for element in train:
  print(element)